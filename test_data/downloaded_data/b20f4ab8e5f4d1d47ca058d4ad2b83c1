{
  "content": "When a whole-body scanner in an airport falls foul of child pornography laws, my immediate thought is that those laws are wrong. Not because I think security is more important than a child's integrity/modesty \u2026 (insert your own nebulous but portentous quality of childhood innocence). Rather, because the law refers to \"indecent\" images of children, and for all naked images of a child to count as \"indecent\", the assumption must be made that adults are as likely as not to be looking at children in a lubricious way. This supposition seems so egregious that it deserves contemplation and debate on its own terms, regardless of the entire issue that raised the spectre of the naked minor in the first place. The least satisfactory response is to simply exclude under-18s from the scans, partly because the alternative is a pat-down, which is surely a more intrusive option. Mainly, though, we can't just unquestioningly roll over to the suggestion that there's a pornographic element to this. That's how ideas that are actually quite extreme pass into accepted truth. The more accessible objections come from groups representing adult civil liberties: Shami Chakrabarti from Liberty is concerned about the lack of guidelines for these scanners. The machines might turn out to be just racial profiling with bells on. Simon Davies from Privacy International, meanwhile, offers this quite different objection. While the American system (and presumably ours) requires images to be instantly deleted, he believes scans of celebrities or people with unusual or freakish body profiles would prove an \"irresistible pull\" for some employees. His sounds like the more marginal concern: sure, I am already irresistibly pulled to have a look at a full body scan of Lady Gaga, and I can't be alone in that, but any lobbying based on the feelings of the famous is inherently ridiculous. The only reason they're so special is that there are so few of them, and it's already built into this conversation that people with a particular aversion to the scans can choose a pat-down. Chakrabarti's concerns have more weight, but don't seem particularly focused on these full-body scanners; any additional security measure would have to be undertaken fairly and without prejudice. So these objections, when you unpick them, consist of one paranoid child-protection agenda, one reasonable but non-specific worry over racial profiling, and one defence of the rights of celebrities. Why is there nothing more full-blooded than this? Being stripped naked is more than a breach of privacy, it has its own footnote in the Geneva conventions. The clash of agendas here is not between security and privacy. Everybody claims to hold security dear to their hearts, and everybody hates it in airports; even those little clear plastic bags annoy me. Such a small gesture, for so much inflight safety, and yet I resent it. But that's not the issue. Imagine if they said: \"For better security, you have to strip naked as in, literally take all your clothes off.\" We would just go by train. There is absolutely no doubt that privacy comes first. No, the complicating factor here is technology. Whole-body imaging, as has been shown in newspapers, is graphic enough to be embarrassing, but still has the grainy feel of an x-ray and, as such, it would seem Luddite and unsophisticated to object. On one hand, technology seems always to work against personal privacy, since it is so tied up with surveillance, and it is always cleverer than us, with our slow-witted fleshbound lumberings. On the other hand, to be against technology feels cranky, old-fashioned, against the tide. Since the privacy argument is mainly, even exclusively, used against technological advance, privacy itself has started to sound like an antiquated, quaint concern. Rope this together with the absurdity of nakedness \u2013 so hard to be urbane or dignified on the matter of whether or not someone's allowed to look through your clothes, for safety reasons \u2013 and you have the perfectly unwinnable argument. It's a universal position \u2013 if any one of us wanted our form entirely visible, we wouldn't bother with clothes at all: they are so pricey and scratchy \u2013 and yet there's no way to defend it. However, this hopelessness is based on the idea that technology necessarily depletes privacy, when of course it is without agenda, and can as easily improve it as destroy it \u2013 or at the very least limit the extent of privacy's destruction. As whole-body imaging became known as a \"virtual strip search\", developers recognised a problem and added modesty algorithms. Put in a modesty filter, and the problem areas disappear; everybody looks like Lara Croft, all collar bone and femur. It's a waste of energy fighting this \u2013 or probably any \u2013 technology. We should just expect and require it to be better, like training a dog.",
  "title": "I'm all for privacy. But I don't want to seem like a Luddite | Zoe Williams",
  "core-words": null,
  "tags": [
    "society/childprotection",
    "society/society",
    "world/privacy",
    "world/surveillance",
    "technology/technology",
    "uk/uksecurity",
    "uk/uk"
  ],
  "lead": "Zoe Williams: Fancy a virtual strip search in the cause of security? Well, no one wants to appear cranky and old-fashioned",
  "id": "commentisfree/libertycentral/2010/jan/06/privacy-luddite-virtual-strip-search",
  "class": null,
  "words": null
}