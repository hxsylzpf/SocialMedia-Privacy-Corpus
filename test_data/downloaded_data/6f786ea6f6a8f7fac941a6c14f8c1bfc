{
  "content": "There's no online platform \u2013 Twitter, Livejournal, Medium, Facebook \u2013 that hasn't been turned, at some point, into a soapbox for ugliness and hate. The platforms usually lay responsibility for this at the feet of their users, and, ultimately, they're right that nobody is to blame for hate speech but the speakers themselves. But do online platforms also have a responsibility for some kind of oversight and moderation? Is a place like Twitter (to choose an obviously successful and public platform) blameless if someone uses it for evil, or does it have a right \u2013 and a duty \u2013 to put its foot down? It comes down to whether online platforms are tools for writing, or more like publications. Nobody blames Microsoft Word if someone uses it to write a threatening letter. But if the New York Times published an opinion piece calling for genocide, there would rightfully be an outcry. Platforms like to think they're more like the former \u2013 just tools, barely, if at all, responsible for monitoring content. But the more successful they are, the more that's wrong. Twitter, for example, is in many ways and for many people the paper of record; it's their first stop for news, opinion, calls to action, personal ads. No, it's not precisely a publication, but it fulfills the role of the daily paper in a way even online papers don't. This power need not come with responsibility; no government entity will force the company to discourage abuse and promote quality, just as nobody can force Twitter to remain to open to hosting hate speech with spurious appeals to the First Amendment. But Twitter \u2013 and other online platforms \u2013 can accept responsibility for the hate speech their platforms enable some users to direct at others, and they should. The legal aspects of \"free speech\" aren't going to help untangle any platform's responsibilities. They are companies, not the governments, and can technically refuse service to anyone at any time. Twitter, for its part, is clear in its terms that it doesn't monitor content and expects users, in large part, to self-police \u2013 but that doesn't mean it can't monitor content, or that doing so would constitute any kind of free speech violation. Twitter obviously prefers to throw up its hands and say You guys are in charge, go nuts, don't break anything, but that's a conscious choice, not a legally-constrained one. If they wanted to be better, they could. But should they? And can they? Twitter and other social media sites move an enormous amount of content. Using an algorithm to scan posts for markers of abuse is possible, but like any algorithm, inevitably imperfect. Relying on user reports is even more flawed, since it leaves the door open for coordinated bullying (like Facebook users reporting drag performers for not using their \"real names\") or for Carrie Nation-style moral crusades (like reporting pictures of breastfeeding). If Twitter \u2013 or any platform \u2013 is more of a publication than a tool, though, the answer is clear: a publication needs editors. And Twitter in particular is huge, so it would need lots and lots and lots of editors. Twitter and Tumblr and Facebook are not newspapers, and in many ways that's good. The internet has been rightly praised for democratizing access and for giving voice to the non-elite; newspapers, with their history of tacitly or overtly amplifying dominant voices, represent the establishment that the internet is breaking down. But \"oversight\" and \"elitism\" are not synonymous. The problem with a masthead, or whatever an internet-platform would call the group of people responsible for making sure their product is not exclusively a playground for the vile, is not that it's a threat to moral anarchy. (It is, but that's not a problem.) The problem is that mastheads tend to reflect the dominant culture. What Twitter needs is improved oversight and diverse oversight reflecting its user base \u2013 not the continued abdication of all oversight. Platforms don't harass people, sure; people harass people. But if people are spewing hate speech using a megaphone you built \u2013 people who wouldn't otherwise be heard \u2013 and you keep handing them megaphones anyway, your megaphone stops being a neutral tool, and your continued provision of it starts to indicate complicity. Hopefully Twitter is starting to take steps to define itself as part of a better online world \u2013 and others will follow.",
  "title": "Twitter need not police hate speech. But it doesn't have to enable it, either | Jess Zimmerman",
  "core-words": null,
  "tags": [
    "technology/twitter",
    "technology/internet",
    "media/blogging",
    "technology/technology",
    "media/digital-media",
    "media/social-media"
  ],
  "lead": "Social media gives people a megaphone. But if companies keep giving that megaphone to trolls intending to hurt people, they start to look complicit",
  "id": "commentisfree/2015/may/27/twitter-police-enable-hate-speech",
  "class": null,
  "words": null
}