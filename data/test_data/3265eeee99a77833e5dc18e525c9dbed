{
  "title": "How Facebook fixed the site: they turned it off and on again. Literally",
  "tags": [
    "technology/facebook",
    "technology/internet",
    "media/socialnetworking",
    "technology/technology"
  ],
  "core-words": {
    "action": 1,
    "go": 2,
    "corresponding": 1,
    "automated": 1,
    "site": 8,
    "normally": 2,
    "server": 2,
    "traceroute": 1,
    "get": 3,
    "course": 1,
    "overwrite": 1,
    "stick": 1,
    "explore": 1,
    "attempt": 2,
    "logistic": 1,
    "cycle": 1,
    "come": 1,
    "button": 1,
    "thing": 1,
    "matter": 1,
    "way": 1,
    "cure": 1,
    "power": 1,
    "value": 7,
    "turn": 3,
    "place": 1,
    "fbnw.net": 1,
    "single": 1,
    "intent": 1,
    "moment": 1,
    "intermediate": 1,
    "time": 1,
    "networking": 1,
    "periodically": 1,
    "transient": 1,
    "correct": 2,
    "gracefully": 1,
    "take": 1,
    "new": 1,
    "deal": 1,
    "date": 1,
    "cluster": 1,
    "error": 1,
    "sort": 1,
    "page": 1,
    "update": 1,
    "central": 1,
    "vanish": 1,
    "spike": 1,
    "store": 1,
    "painful": 1,
    "query": 1,
    "literally": 1,
    "invalid": 2,
    "giant": 1,
    "sit": 2,
    "persistent": 1,
    "misbehave": 1,
    "bad": 1,
    "ring": 1,
    "replace": 1,
    "network": 4,
    "unusual": 1,
    "social": 1,
    "stop": 2,
    "god": 1,
    "design": 2,
    "oh": 1,
    "pass": 1,
    "fix": 1,
    "client": 1,
    "pattern": 1,
    "show": 1,
    "facebook": 1,
    "cache": 4,
    "configuration": 3,
    "interpret": 1,
    "serve": 1,
    "tfbnw.net": 1,
    "problem": 3,
    "pc": 1,
    "run": 2,
    "feedback": 2,
    "traffic": 1,
    "like": 3,
    "vast": 1,
    "wrong": 2,
    "check": 1,
    "belly": 1,
    "database": 3,
    "call": 1,
    "centre": 1,
    "delete": 1,
    "system": 5,
    "usually": 1,
    "connect": 1,
    "updated": 1,
    "key": 1,
    "follow": 1,
    "loop": 1,
    "effect": 1,
    "mean": 2
  },
  "class": null,
  "id": "technology/blog/2010/sep/24/facebook-outage-turned-off-on-again",
  "words": {
    "action": 1,
    "shamefac": 1,
    "deal": 1,
    "performance": 1,
    "involve": 1,
    "normally": 2,
    "server": 4,
    "caching": 1,
    "traceroute": 1,
    "get": 4,
    "overwrite": 1,
    "overwhelm": 2,
    "circle": 1,
    "pass": 1,
    "software": 1,
    "button": 1,
    "matter": 1,
    "slightly": 1,
    "way": 1,
    "cure": 1,
    "result": 1,
    "power": 1,
    "persistent": 3,
    "basically": 1,
    "technical": 1,
    "transient": 2,
    "tell": 1,
    "copy": 1,
    "reliability": 1,
    "time": 2,
    "networking": 1,
    "lot": 1,
    "recover": 2,
    "site": 13,
    "take": 1,
    "new": 1,
    "reach": 1,
    "error": 2,
    "hour": 1,
    "gracefully": 1,
    "long": 1,
    "engineering": 1,
    "painful": 1,
    "query": 5,
    "replicate": 1,
    "invalid": 5,
    "make": 1,
    "enter": 1,
    "phone": 1,
    "quickly": 1,
    "find": 1,
    "justify": 1,
    "network": 4,
    "unusual": 2,
    "slowly": 1,
    "able": 1,
    "reserve": 1,
    "support": 1,
    "stop": 2,
    "attempt": 3,
    "design": 2,
    "seriously": 1,
    "sir": 1,
    "come": 1,
    "director": 1,
    "see": 1,
    "show": 1,
    "corresponding": 1,
    "facebook": 3,
    "cache": 5,
    "know": 1,
    "run": 3,
    "problem": 7,
    "database": 7,
    "work": 2,
    "blogpost": 1,
    "feedback": 3,
    "traffic": 1,
    "ad": 2,
    "logistic": 1,
    "service": 1,
    "mr": 1,
    "check": 1,
    "enormous": 1,
    "offline": 1,
    "people": 1,
    "flaw": 1,
    "usually": 1,
    "connect": 1,
    "condition": 1,
    "updated": 1,
    "start": 1,
    "key": 2,
    "want": 1,
    "operation": 1,
    "effect": 1,
    "damage": 1,
    "go": 4,
    "continue": 1,
    "automated": 2,
    "periodically": 1,
    "fbnw.net": 1,
    "change": 2,
    "course": 2,
    "stick": 1,
    "explore": 1,
    "end": 1,
    "vast": 1,
    "request": 2,
    "like": 4,
    "thing": 2,
    "handling": 1,
    "correct": 2,
    "value": 10,
    "turn": 5,
    "place": 2,
    "particular": 1,
    "single": 2,
    "intent": 1,
    "happen": 1,
    "moment": 1,
    "intermediate": 1,
    "sell": 2,
    "oh": 1,
    "cycle": 1,
    "cause": 4,
    "date": 1,
    "inside": 1,
    "sort": 3,
    "page": 1,
    "update": 1,
    "central": 2,
    "content": 1,
    "vanish": 1,
    "stream": 1,
    "spike": 1,
    "root": 1,
    "store": 2,
    "big": 1,
    "literally": 2,
    "fail": 1,
    "giant": 1,
    "sit": 2,
    "verify": 1,
    "misbehave": 1,
    "word": 1,
    "severe": 1,
    "ring": 1,
    "replace": 1,
    "explain": 2,
    "wo": 1,
    "god": 1,
    "valuation": 1,
    "cluster": 3,
    "fix": 7,
    "client": 2,
    "pattern": 1,
    "bad": 2,
    "configuration": 5,
    "follow": 1,
    "interpret": 2,
    "serve": 1,
    "tfbnw.net": 1,
    "pc": 1,
    "right": 1,
    "social": 1,
    "mean": 5,
    "unfortunate": 1,
    "original": 1,
    "belly": 1,
    "outage": 3,
    "call": 1,
    "centre": 1,
    "delete": 1,
    "system": 7,
    "wrong": 3,
    "photo": 1,
    "apologize": 1,
    "loop": 2,
    "allow": 2
  },
  "content": "I found it, Mr Zuckerberg! Photo by Sir Mildred Pierce on Flickr. Some rights reserved Ever been on the phone to IT support and they told you to turn it off and then on again, and that sorts it out? Facebook last night had that sort of problem. So they turned the site off and on again. And it fixed their problem. Literally. As Robert Johnson, its director of software engineering, explained in a slightly shamefaced blogpost, the site was offline for about two-and-a-half hours \u2013 its worst outage in four years \u2013 due to some technical changes that Facebook had made. It wasn't only the site itself which went belly-up; the Like buttons (which connect back to Facebook) vanished on 350,000 sites too, and the API which powers its OpenGraph system had serious problems. The logistics of running a vast network like Facebook mean that you don't stick all your servers in a single place, of course. Facebook runs a big caching operation, so that lots of servers replicate its content. The cache gets updated periodically; it sits on a network called tfbnw.net (for \"the Facebook network\": you can see it here in this traceroute to Facebook, which shows what the intermediate networks are between one site and Facebook), which in effect sits like a ring around the \"central\" Facebook site. Sometimes, things go wrong in the cache as values go out of date; but that's no problem, usually, because you can overwrite them with correct values from the centre. At least, you would like to. Here's how Johnson explained it: \"The key flaw that caused this outage to be so severe was an unfortunate handling of an error condition. An automated system for verifying configuration values ended up causing much more damage than it fixed. \"The intent of the automated system is to check for configuration values that are invalid in the cache and replace them with updated values from the persistent store. This works well for a transient problem with the cache, but it doesn't work when the persistent store is invalid.\" In other words: something went wrong inside the circle. And that wrong value got passed out to all the fbnw.net servers that would normally serve up Facebook pages. Back to Johnson: \"Today we made a change to the persistent copy of a configuration value that was interpreted as invalid. This meant that every single client saw the invalid value and attempted to fix it. Because the fix involves making a query to a cluster of databases, that cluster was quickly overwhelmed by hundreds of thousands of queries a second.\" Basically, tfbnw.net's servers started querying the central system all at once, which overwhelmed it. \"To make matters worse, every time a client got an error attempting to query one of the databases it interpreted it as an invalid value, and deleted the corresponding cache key. This meant that even after the original problem had been fixed, the stream of queries continued. As long as the databases failed to service some of the requests, they were causing even more requests to themselves. We had entered a feedback loop that didn't allow the databases to recover.\" And now we come to the \"oh my god, we're really going to have to do that?\" moment: \"The way to stop the feedback cycle was quite painful \u2013 we had to stop all traffic to this database cluster, which meant turning off the site. Once the databases had recovered and the root cause had been fixed, we slowly allowed more people back onto the site.\" And the result? \"This got the site back up and running today, and for now we've turned off the system that attempts to correct configuration values. We're exploring new designs for this configuration system following design patterns of other systems at Facebook that deal more gracefully with feedback loops and transient spikes.\" That means that there may be some times over the next few days when you won't be able to reach Facebook in particular places, or that unusual things will happen. \"We apologize again for the site outage, and we want you to know that we take the performance and reliability of Facebook very seriously.\" Well, of course: if the site's down, it can't sell ads, and if it can't sell ads, how is Mark Zuckerberg going to justify his enormous Forbes valuation?",
  "lead": "An unusual database problem at the giant social networking site could only be cured by taking the sort of action you normally take with a misbehaving PC"
}